source("~/.active-rstudio-document", echo=TRUE)
colnames(ircc_peel)
view(healthline)
# -----------------------------------------------------------------------------
#
# Scrape organization service information from Healthline service directory
# Jason Jiang - Created: 2022/Nov/30
#               Last edited: 2022/Dec/30
#
# UTM - CRAWL
#
# -----------------------------------------------------------------------------
library(tidyverse)
library(rvest)  # for web scraping
###############################################################################
## PART 1: GET LINKS TO ORGANIZATION SERVICE PAGES
###############################################################################
# Define websites to mine service links from
SERVICE_WEBSITES <-
c('https://www.centralwesthealthline.ca/listservices.aspx?id=10566',
'https://www.torontocentralhealthline.ca/listServices.aspx?id=10566')
###############################################################################
# Helper functions
get_base_url <- function(site) {
# ---------------------------------------------------------------------------
# Extracts the base domain for a website.
#
# ex:
# get_base_url('https://www.google.com/foo/bar') -> 'https://www.google.com'
# get_base_url('https://www.google.com') -> 'https://www.google.com'
# ---------------------------------------------------------------------------
return(str_extract(site, '^https://www\\.[[A-Za-z]]+\\.[[A-Za-z]]{2,3}'))
}
###############################################################################
# Extract organization names + links from each website in SERVICE_WEBSITES
# Initialize dataframe for holding organization links
# Also initialize columns for holding web-scraped information about these
# organizations
# The columns for scraped info will all be NA until Part 2 of the code.
organization_info_df <- data.frame(matrix(ncol = 10, nrow = 0))
colnames(organization_info_df) <- c('Name', 'Healthline Link', 'Website', 'Address',
'Email', 'Phone', 'Description', 'Fees', 'Eligibility',
'Languages', 'Areas Served', 'Last Updated')
# -----------------------------------------------------------------------------
#
# Scrape organization service information from Healthline service directory
# Jason Jiang - Created: 2022/Nov/30
#               Last edited: 2022/Dec/30
#
# UTM - CRAWL
#
# -----------------------------------------------------------------------------
library(tidyverse)
library(rvest)  # for web scraping
###############################################################################
## PART 1: GET LINKS TO ORGANIZATION SERVICE PAGES
###############################################################################
# Define websites to mine service links from
SERVICE_WEBSITES <-
c('https://www.centralwesthealthline.ca/listservices.aspx?id=10566',
'https://www.torontocentralhealthline.ca/listServices.aspx?id=10566')
###############################################################################
# Helper functions
get_base_url <- function(site) {
# ---------------------------------------------------------------------------
# Extracts the base domain for a website.
#
# ex:
# get_base_url('https://www.google.com/foo/bar') -> 'https://www.google.com'
# get_base_url('https://www.google.com') -> 'https://www.google.com'
# ---------------------------------------------------------------------------
return(str_extract(site, '^https://www\\.[[A-Za-z]]+\\.[[A-Za-z]]{2,3}'))
}
###############################################################################
# Extract organization names + links from each website in SERVICE_WEBSITES
# Initialize dataframe for holding organization links
# Also initialize columns for holding web-scraped information about these
# organizations
# The columns for scraped info will all be NA until Part 2 of the code.
organization_info_df <- data.frame(matrix(ncol = 12, nrow = 0))
colnames(organization_info_df) <- c('Name', 'Healthline Link', 'Website', 'Address',
'Email', 'Phone', 'Description', 'Fees', 'Eligibility',
'Languages', 'Areas Served', 'Last Updated')
for (site in SERVICE_WEBSITES) {
# get base domain for the website
base_url <- get_base_url(site)
# load in HTML for the website
site <- read_html(site)
# HTML nodes in website that *may* contain links to organizations
candidate_nodes <- site %>%
html_nodes('td') %>%
html_nodes('a')
# id attributes for such HTML nodes
candidate_nodes_ids <- candidate_nodes %>%
html_attr('id')
# indices for such HTML nodes having id attributes corresponding to
# organizations
links_idx <-
which(!is.na(candidate_nodes_ids) & str_detect(candidate_nodes_ids, '.+_lnkService$'))
# get full links to organizations from the HTML nodes corresponding to
# organizations
links <- unname(sapply((candidate_nodes %>% html_attr('href'))[links_idx],
function(s) {str_c(base_url, '/', s)}))
# get names for the organizations each link redirects to
organization_names <- candidate_nodes[links_idx] %>% html_text()
# add the extracted service names and links for this website to
# service_links_df
organization_info_df <- rbind(organization_info_df,
# create "mini-dataframe" holding extracted
# organization names + links to append to
# organization_info_df
data.frame('Name' = organization_names,
'Healthline Link' = links,
'Website' = NA,
'Address' = NA,
'Email' = NA,
'Phone' = NA,
'Description' = NA,
'Fees' = NA,
'Eligibility' = NA,
'Languages' = NA,
"Areas Served" = NA,
"Last Updated" = NA))
}
# Remove unnecessary variables from environment
rm(candidate_nodes, site, base_url, candidate_nodes_ids, links, links_idx,
organization_names)
view(organization_info_df)
# -----------------------------------------------------------------------------
#
# Scrape organization service information from Healthline service directory
# Jason Jiang - Created: 2022/Nov/30
#               Last edited: 2022/Dec/30
#
# UTM - CRAWL
#
# -----------------------------------------------------------------------------
library(tidyverse)
library(rvest)  # for web scraping
###############################################################################
## PART 1: GET LINKS TO ORGANIZATION SERVICE PAGES
###############################################################################
# Define websites to mine service links from
SERVICE_WEBSITES <-
c('https://www.centralwesthealthline.ca/listservices.aspx?id=10566',
'https://www.torontocentralhealthline.ca/listServices.aspx?id=10566')
###############################################################################
# Helper functions
get_base_url <- function(site) {
# ---------------------------------------------------------------------------
# Extracts the base domain for a website.
#
# ex:
# get_base_url('https://www.google.com/foo/bar') -> 'https://www.google.com'
# get_base_url('https://www.google.com') -> 'https://www.google.com'
# ---------------------------------------------------------------------------
return(str_extract(site, '^https://www\\.[[A-Za-z]]+\\.[[A-Za-z]]{2,3}'))
}
###############################################################################
# Extract organization names + links from each website in SERVICE_WEBSITES
# Initialize dataframe for holding organization links
# Also initialize columns for holding web-scraped information about these
# organizations
# The columns for scraped info will all be NA until Part 2 of the code.
organization_info_df <- data.frame(matrix(ncol = 12, nrow = 0))
colnames(organization_info_df) <- c('Name', 'Healthline Link', 'Website', 'Address',
'Email', 'Phone', 'Description', 'Fees', 'Eligibility',
'Languages', 'Areas Served', 'Last Updated')
for (site in SERVICE_WEBSITES) {
# get base domain for the website
base_url <- get_base_url(site)
# load in HTML for the website
site <- read_html(site)
# HTML nodes in website that *may* contain links to organizations
candidate_nodes <- site %>%
html_nodes('td') %>%
html_nodes('a')
# id attributes for such HTML nodes
candidate_nodes_ids <- candidate_nodes %>%
html_attr('id')
# indices for such HTML nodes having id attributes corresponding to
# organizations
links_idx <-
which(!is.na(candidate_nodes_ids) & str_detect(candidate_nodes_ids, '.+_lnkService$'))
# get full links to organizations from the HTML nodes corresponding to
# organizations
links <- unname(sapply((candidate_nodes %>% html_attr('href'))[links_idx],
function(s) {str_c(base_url, '/', s)}))
# get names for the organizations each link redirects to
organization_names <- candidate_nodes[links_idx] %>% html_text()
# add the extracted service names and links for this website to
# service_links_df
organization_info_df <- rbind(organization_info_df,
# create "mini-dataframe" holding extracted
# organization names + links to append to
# organization_info_df
data.frame('Name' = organization_names,
`Healthline Link` = links,
'Website' = NA,
'Address' = NA,
'Email' = NA,
'Phone' = NA,
'Description' = NA,
'Fees' = NA,
'Eligibility' = NA,
'Languages' = NA,
`Areas Served` = NA,
`Last Updated` = NA))
}
# Remove unnecessary variables from environment
rm(candidate_nodes, site, base_url, candidate_nodes_ids, links, links_idx,
organization_names)
view(organization_info_df)
organization_info_df['Healthline Link']
organization_info_df[['Healthline Link']]
organization_info_df[['Healthline.Link']]
# -----------------------------------------------------------------------------
#
# Scrape organization service information from Healthline service directory
# Jason Jiang - Created: 2022/Nov/30
#               Last edited: 2022/Dec/30
#
# UTM - CRAWL
#
# -----------------------------------------------------------------------------
library(tidyverse)
library(rvest)  # for web scraping
###############################################################################
## PART 1: GET LINKS TO ORGANIZATION SERVICE PAGES
###############################################################################
# Define websites to mine service links from
SERVICE_WEBSITES <-
c('https://www.centralwesthealthline.ca/listservices.aspx?id=10566',
'https://www.torontocentralhealthline.ca/listServices.aspx?id=10566')
###############################################################################
# Helper functions
get_base_url <- function(site) {
# ---------------------------------------------------------------------------
# Extracts the base domain for a website.
#
# ex:
# get_base_url('https://www.google.com/foo/bar') -> 'https://www.google.com'
# get_base_url('https://www.google.com') -> 'https://www.google.com'
# ---------------------------------------------------------------------------
return(str_extract(site, '^https://www\\.[[A-Za-z]]+\\.[[A-Za-z]]{2,3}'))
}
###############################################################################
# Extract organization names + links from each website in SERVICE_WEBSITES
# Initialize dataframe for holding organization links
# Also initialize columns for holding web-scraped information about these
# organizations
# The columns for scraped info will all be NA until Part 2 of the code.
organization_info_df <- data.frame(matrix(ncol = 12, nrow = 0))
colnames(organization_info_df) <- c('Name', 'Healthline Link', 'Website', 'Address',
'Email', 'Phone', 'Description', 'Fees', 'Eligibility',
'Languages', 'Areas Served', 'Last Updated')
for (site in SERVICE_WEBSITES) {
# get base domain for the website
base_url <- get_base_url(site)
# load in HTML for the website
site <- read_html(site)
# HTML nodes in website that *may* contain links to organizations
candidate_nodes <- site %>%
html_nodes('td') %>%
html_nodes('a')
# id attributes for such HTML nodes
candidate_nodes_ids <- candidate_nodes %>%
html_attr('id')
# indices for such HTML nodes having id attributes corresponding to
# organizations
links_idx <-
which(!is.na(candidate_nodes_ids) & str_detect(candidate_nodes_ids, '.+_lnkService$'))
# get full links to organizations from the HTML nodes corresponding to
# organizations
links <- unname(sapply((candidate_nodes %>% html_attr('href'))[links_idx],
function(s) {str_c(base_url, '/', s)}))
# get names for the organizations each link redirects to
organization_names <- candidate_nodes[links_idx] %>% html_text()
# add the extracted service names and links for this website to
# service_links_df
organization_info_df <- rbind(organization_info_df,
# create "mini-dataframe" holding extracted
# organization names + links to append to
# organization_info_df
data.frame('Name' = organization_names,
'Healthline Link' = links,
'Website' = NA,
'Address' = NA,
'Email' = NA,
'Phone' = NA,
'Description' = NA,
'Fees' = NA,
'Eligibility' = NA,
'Languages' = NA,
"Areas Served" = NA,
"Last Updated" = NA,
check.names=FALSE))
}
# Remove unnecessary variables from environment
rm(candidate_nodes, site, base_url, candidate_nodes_ids, links, links_idx,
organization_names)
view(organization_info_df)
IDS_OF_INTEREST <- list('Address' = 'ctl00_ContentPlaceHolder1_lblAddress',
'Description' = 'ctl00_ContentPlaceHolder1_lblDescription',
'Fees' = 'ctl00_ContentPlaceHolder1_lblFees',
'Eligibility' = 'ctl00_ContentPlaceHolder1_lblEligibility',
'Languages' = 'ctl00_ContentPlaceHolder1_lblLanguages',
'Areas Served' = 'ctl00_ContentPlaceHolder1_lblAreasServed',
'Last Updated' = 'ctl00_ContentPlaceHolder1_lblLastUpdated',
'Email' = '',
'Phone' = '')
i = 1
org_site <- read_html(organization_info_df[['Healthline Link']][i])
# Get HTML nodes for spans, which contain all the attributes we want to scrape
spans <- org_site %>%
html_nodes('span')
spans_ids <- spans %>%
html_attr('id')
spans_ids
spans_ids[str_detect(spans_ids, 'Email')]
spans_ids[which(str_detect(spans_ids, 'Email'))]
spans_ids[which(str_detect(spans_ids, 'Phone'))]
